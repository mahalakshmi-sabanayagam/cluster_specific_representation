{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7508dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e03c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from src.data import parallel_line, orthogonal, triangle, lines_3D, real_data_loader, add_noise_data\n",
    "import math, csv\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e4c38b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import Autoencoder, CNN_Autoencoder\n",
    "from src.train import train_AE, train_TAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c468c4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4961a5",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3aeaa5",
   "metadata": {},
   "source": [
    "### synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e775429a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dataset:  torch.Size([300, 6])\n",
      "3\n",
      "kmeans  0.6488571581544602\n",
      "Number of parameters in AE:  12\n",
      "AE1  0.5404621785177707\n",
      "Number of parameters in AE:  28\n",
      "AE2  0.5404621785177707\n",
      "Number of parameters in AE:  36\n",
      "AE3  0.7525664019930466\n",
      "Number of parameters in TAE:  36\n",
      "epoch 149 loss 0.47929306640289726\n",
      "TAE1  0.5931258064824382\n",
      "Number of parameters in TAE:  84\n",
      "epoch 149 loss 0.33466492355335503\n",
      "TAE2  0.5459658056596445\n",
      "Number of parameters in TAE:  36\n",
      "epoch 149 loss 0.3701238912646659\n",
      "PTAE  0.6683018797932874\n",
      "Number of parameters in AE:  12\n",
      "AE1  0.5404621785177707\n",
      "Number of parameters in AE:  28\n",
      "AE2  0.5404621785177707\n",
      "Number of parameters in AE:  36\n",
      "AE3  0.5404621785177707\n",
      "Number of parameters in TAE:  36\n",
      "epoch 149 loss 0.08245556292124093\n",
      "TAE1  1.0\n",
      "Number of parameters in TAE:  84\n",
      "epoch 149 loss 0.027487855409660065\n",
      "TAE2  1.0\n",
      "Number of parameters in TAE:  36\n",
      "epoch 149 loss 0.1907978732076784\n",
      "PTAE  1.0\n",
      "Number of parameters in AE:  12\n",
      "AE1  0.49200971858993836\n",
      "Number of parameters in AE:  28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1061906/351470483.py:68: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
      "  kmeans = KMeans(n_clusters=n_clusters, random_state=0,n_init=10).fit(X_embed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE2  0.0\n",
      "Number of parameters in AE:  36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1061906/351470483.py:84: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
      "  kmeans = KMeans(n_clusters=n_clusters, random_state=0,n_init=10).fit(X_embed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE3  0.0\n",
      "Number of parameters in TAE:  36\n",
      "epoch 149 loss 0.018861024471892356\n",
      "TAE1  1.0\n",
      "Number of parameters in TAE:  84\n",
      "epoch 149 loss 0.00949009539016212\n",
      "TAE2  1.0\n",
      "Number of parameters in TAE:  36\n",
      "epoch 149 loss 0.17938635345334963\n",
      "PTAE  1.0\n"
     ]
    }
   ],
   "source": [
    "# kmeans_ari, ae_ari, tae_ari gives the ari of respective algorithms\n",
    "# hyperparameters: repetition, epochs, lr, reg,  \n",
    "repetition = 5\n",
    "\n",
    "np.random.seed(24)\n",
    "results_dict = dict()\n",
    "\n",
    "csv_file = \"results/ae_clust_synthetic.csv\"\n",
    "if not os.path.isfile(csv_file):\n",
    "    fields = [\"data\", \"rep\", \"X\", \"Y\", \"shape\", \"noise\",\n",
    "            \"kmeans_labels\", \"kmeans_ari\",\n",
    "            \"ae1_num_params\", \"ae1_lr\", \"ae1_epochs\", \"ae1_final_train_loss\", \"ae1_labels\", \"ae1_ari\", \n",
    "            \"ae2_num_params\", \"ae2_lr\", \"ae2_epochs\", \"ae2_final_train_loss\", \"ae2_labels\", \"ae2_ari\", \n",
    "            \"ae3_num_params\", \"ae3_lr\", \"ae3_epochs\", \"ae3_final_train_loss\", \"ae3_labels\", \"ae3_ari\",\n",
    "            \"tae1_num_params\", \"tae1_lr\", \"tae1_epochs\", \"tae1_final_train_loss\", \"tae1_labels\", \"tae1_ari\",\n",
    "            \"tae2_num_params\", \"tae2_lr\", \"tae2_epochs\", \"tae2_final_train_loss\", \"tae2_labels\", \"tae2_ari\",\n",
    "            \"ptae_num_params\", \"ptae_lr\", \"ptae_epochs\", \"ptae_final_train_loss\", \"ptae_labels\", \"ptae_ari\", \n",
    "            ]\n",
    "    with open(csv_file, \"a\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(fields)\n",
    "\n",
    "lr_l = [0.001, 0.01, 0.1]\n",
    "epochs = 150\n",
    "for (idx, dataset, data_generation) in [\n",
    "                            (0, 'parallel_line', parallel_line(noise=0)), \n",
    "                            (1, 'orthogonal', orthogonal(noise=0)), \n",
    "                            (2, 'triangle', triangle(noise=0)), \n",
    "                            (3, 'lines_3D', lines_3D(noise=0))\n",
    "                             ]:\n",
    "    print(\"shape of dataset: \", data_generation[0].shape)\n",
    "    for rep in range(repetition):\n",
    "        print(rep)\n",
    "        X,Y,X_noise,n_clusters = data_generation\n",
    "        x_idx = torch.tensor(np.arange(X.shape[0]))\n",
    "        shuffle_idx = torch.randperm(x_idx.shape[0])\n",
    "        x_idx = x_idx[shuffle_idx]\n",
    "        X = X[x_idx].float().to(device)\n",
    "        Y = Y[x_idx].float().to(device)\n",
    "        \n",
    "        # k means\n",
    "        k = KMeans(n_clusters=n_clusters, random_state=rep, n_init=10).fit(X.cpu())\n",
    "        k_ari = adjusted_rand_score(k.labels_, Y.cpu())\n",
    "        print('kmeans ', k_ari)\n",
    "         \n",
    "        for lr in lr_l:\n",
    "            res = [dataset, rep, X.cpu().detach().numpy().tolist(), Y.cpu().detach().numpy().tolist(), list(data_generation[0].shape), 0,\n",
    "                  k.labels_.tolist(), k_ari]\n",
    "            \n",
    "            # Standard one layer AE1 \n",
    "            net = Autoencoder(in_feature=X.shape[1],embed_l=[1]).to(device)\n",
    "            ae_l = train_AE(net,X.clone(),Y.clone(), lr=lr,epochs=epochs)\n",
    "            X_embed = net.get_embedding(X).cpu().detach().numpy()\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=0,n_init=10).fit(X_embed)\n",
    "            AE_clust_assign = kmeans.labels_\n",
    "            ari = adjusted_rand_score(AE_clust_assign, Y.cpu())\n",
    "            ae1_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(ae1_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(ae_l[-1])\n",
    "            res.append(AE_clust_assign.tolist())\n",
    "            res.append(ari)\n",
    "            print('AE1 ', ari)\n",
    "\n",
    "            # Standard two layer AE2 \n",
    "            net = Autoencoder(in_feature=X.shape[1],embed_l=[2,1]).to(device)\n",
    "            ae_l = train_AE(net,X.clone(),Y.clone(), lr=lr,epochs=epochs)\n",
    "            X_embed = net.get_embedding(X).cpu().detach().numpy()\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=0,n_init=10).fit(X_embed)\n",
    "            AE_clust_assign = kmeans.labels_\n",
    "            ari = adjusted_rand_score(AE_clust_assign, Y.cpu())\n",
    "            ae2_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(ae2_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(ae_l[-1])\n",
    "            res.append(AE_clust_assign.tolist())\n",
    "            res.append(ari)\n",
    "            print('AE2 ', ari)\n",
    "\n",
    "            # Standard two layer AE3 \n",
    "            net = Autoencoder(in_feature=X.shape[1],embed_l=[2,1*n_clusters]).to(device)\n",
    "            ae_l = train_AE(net,X.clone(),Y.clone(), lr=lr,epochs=epochs)\n",
    "            X_embed = net.get_embedding(X).cpu().detach().numpy()\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=0,n_init=10).fit(X_embed)\n",
    "            AE_clust_assign = kmeans.labels_\n",
    "            ari = adjusted_rand_score(AE_clust_assign, Y.cpu())\n",
    "            ae3_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(ae3_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(ae_l[-1])\n",
    "            res.append(AE_clust_assign.tolist())\n",
    "            res.append(ari)\n",
    "            print('AE3 ', ari)\n",
    "\n",
    "            # Tensorized AE1 - one layer\n",
    "            net, tae_l, TAE_clust_assign, _, _, _ = train_TAE(X.clone(),Y.clone(), n_clusters=n_clusters, lr=lr, reg=0.1, embed_l=[1], shared=False, epochs=epochs)\n",
    "            TAE_clust_assign = torch.argmax(TAE_clust_assign,axis=0).cpu().detach().numpy()\n",
    "            ari = adjusted_rand_score(TAE_clust_assign, Y.cpu())\n",
    "            tae1_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(tae1_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(tae_l[-1])\n",
    "            res.append(TAE_clust_assign.tolist())\n",
    "            res.append(ari)\n",
    "            print('TAE1 ', ari)\n",
    "\n",
    "            # Tensorized AE2 - two layer\n",
    "            net, tae_l, TAE_clust_assign, _, _, _ = train_TAE(X.clone(),Y.clone(), n_clusters=n_clusters, lr=lr, reg=0.1, embed_l=[2,1], shared=False, epochs=epochs)\n",
    "            TAE_clust_assign = torch.argmax(TAE_clust_assign,axis=0).cpu().detach().numpy()\n",
    "            ari = adjusted_rand_score(TAE_clust_assign, Y.cpu())\n",
    "            tae2_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(tae2_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(tae_l[-1])\n",
    "            res.append(TAE_clust_assign.tolist())\n",
    "            res.append(ari)\n",
    "            print('TAE2 ', ari)\n",
    "\n",
    "            # Shared TAE\n",
    "            net, shared_l, SHARED_clust_assign, _, _, _ = train_TAE(X.clone(),Y.clone(), n_clusters=n_clusters, lr=lr, reg=0.1, epochs=epochs, embed_l=[2,1], shared=True)\n",
    "            PTAE_clust_assign = torch.argmax(SHARED_clust_assign,axis=0).cpu().detach().numpy()\n",
    "            ari = adjusted_rand_score(PTAE_clust_assign, Y.cpu())\n",
    "            ptae_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(ptae_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(shared_l[-1])\n",
    "            res.append(PTAE_clust_assign.tolist())\n",
    "            res.append(ari)\n",
    "            print('PTAE ', ari)\n",
    "\n",
    "            with open(csv_file, \"a\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9391d7b",
   "metadata": {},
   "source": [
    "### real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1024159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST loaded dataset Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n",
      "Number of total samples  torch.Size([1000])\n",
      "shape of dataset:  torch.Size([1000, 784])\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/lxc0D/apdl004/cluster_specific_representation/cluster_ae/src/data.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X_full[x_idx], dtype=torch.float)\n",
      "/dss/dsshome1/lxc0D/apdl004/cluster_specific_representation/cluster_ae/src/data.py:125: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(Y_full[x_idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans  0.7591186456275665\n",
      "Number of parameters in AE:  1568\n",
      "AE1  0.12076698526179401\n",
      "Number of parameters in AE:  3140\n",
      "AE2  0.11831033074959062\n",
      "Number of parameters in AE:  3156\n",
      "AE3  0.34020411357345626\n",
      "Number of parameters in TAE:  7840\n",
      "epoch 199 loss 3122.494755859375\n",
      "TAE1  0.7491182910487174\n",
      "Number of parameters in TAE:  15700\n",
      "epoch 199 loss 3082.739341796875\n",
      "TAE2  0.7451978948793391\n",
      "Number of parameters in TAE:  3156\n",
      "epoch 199 loss 3102.813781311035\n",
      "PTAE  0.7273294581633553\n",
      "Number of parameters in AE:  1568\n",
      "AE1  0.08715047243685645\n",
      "Number of parameters in AE:  3140\n",
      "AE2  0.11818703868753769\n",
      "Number of parameters in AE:  3156\n",
      "AE3  0.3034190544868286\n",
      "Number of parameters in TAE:  7840\n",
      "epoch 199 loss 2761.672920959473\n",
      "TAE1  0.8244280111556831\n",
      "Number of parameters in TAE:  15700\n",
      "epoch 199 loss 2686.8864410400392\n",
      "TAE2  0.7980636861144129\n",
      "Number of parameters in TAE:  3156\n",
      "epoch 199 loss 2848.5042033081054\n",
      "PTAE  0.7199246207376919\n",
      "Number of parameters in AE:  1568\n",
      "AE1  0.01749006628801538\n",
      "Number of parameters in AE:  3140\n",
      "AE2  0.19216238375032796\n",
      "Number of parameters in AE:  3156\n",
      "AE3  0.08209939552329082\n",
      "Number of parameters in TAE:  7840\n",
      "epoch 199 loss 3550.671024169922\n",
      "TAE1  0.37515368588981585\n",
      "Number of parameters in TAE:  15700\n",
      "epoch 199 loss 3427.350259765625\n",
      "TAE2  0.33863254458282827\n",
      "Number of parameters in TAE:  3156\n",
      "epoch 199 loss 2880.542643280029\n",
      "PTAE  0.7940825846173719\n"
     ]
    }
   ],
   "source": [
    "# kmeans_ari, ae_ari, tae_ari gives the ari of respective algorithms\n",
    "# hyperparameters: repetition, epochs, lr, reg,  \n",
    "repetition = 5\n",
    "results_dict = dict()\n",
    "\n",
    "csv_file = \"results/ae_clust_real.csv\"\n",
    "if not os.path.isfile(csv_file):\n",
    "    fields = [\"data\", \"rep\", \"X\", \"Y\", \"shape\", \"noise\",\n",
    "             \"kmeans_labels\", \"kmeans_ari\",\n",
    "             \"ae1_num_params\", \"ae1_lr\", \"ae1_epochs\", \"ae1_final_train_loss\", \"ae1_labels\", \"ae1_ari\", \n",
    "             \"ae2_num_params\", \"ae2_lr\", \"ae2_epochs\", \"ae2_final_train_loss\", \"ae2_labels\", \"ae2_ari\", \n",
    "             \"ae3_num_params\", \"ae3_lr\", \"ae3_epochs\", \"ae3_final_train_loss\", \"ae3_labels\", \"ae3_ari\",\n",
    "             \"tae1_num_params\", \"tae1_lr\", \"tae1_epochs\", \"tae1_final_train_loss\", \"tae1_labels\", \"tae1_ari\",\n",
    "             \"tae2_num_params\", \"tae2_lr\", \"tae2_epochs\", \"tae2_final_train_loss\", \"tae2_labels\", \"tae2_ari\",\n",
    "             \"ptae_num_params\", \"ptae_lr\", \"ptae_epochs\", \"ptae_final_train_loss\", \"ptae_labels\", \"ptae_ari\", \n",
    "             ]\n",
    "    with open(csv_file, \"a\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(fields)\n",
    "\n",
    "lr_l = [0.001, 0.01, 0.1]\n",
    "epochs = 200\n",
    "for dataset in ['mnist', 'penguin4', 'iris']:\n",
    "    if dataset == 'penguin4':\n",
    "        X,Y,X_noise,n_clusters = real_data_loader(dataset='penguin4', class_labels_list=[0,1,2], normalise_data=True, subsample=False, noise=0)\n",
    "    elif dataset == 'iris':\n",
    "        X,Y,X_noise,n_clusters = real_data_loader(dataset='iris', class_labels_list=[0,1,2], normalise_data=False, subsample=False, noise=0)\n",
    "    elif dataset == 'mnist':\n",
    "        X,Y,X_noise,n_clusters = real_data_loader(dataset='mnist', class_labels_list=[0,1,2,3,4], num_samples=200, normalise_data=False, subsample=True, noise=0)\n",
    "    print(\"shape of dataset: \", X.shape)\n",
    "    for rep in range(repetition):\n",
    "        np.random.seed(rep)\n",
    "        print(rep)\n",
    "#         X,Y,X_noise,n_clusters = data_generation\n",
    "        x_idx = torch.tensor(np.arange(X.shape[0]))\n",
    "        shuffle_idx = torch.randperm(x_idx.shape[0])\n",
    "        x_idx = x_idx[shuffle_idx]\n",
    "        X = X[x_idx].float().to(device)\n",
    "        Y = Y[x_idx].float().to(device)\n",
    "        \n",
    "        # k means\n",
    "        k = KMeans(n_clusters=n_clusters, random_state=rep, n_init=rep+1).fit(X.cpu())\n",
    "        k_ari = adjusted_rand_score(k.labels_, Y.cpu())\n",
    "        print('kmeans ', k_ari)\n",
    "         \n",
    "        for lr in lr_l:\n",
    "            res = [dataset, rep, X.cpu().detach().numpy().tolist(), Y.cpu().detach().numpy().tolist(), list(X.shape), 0,\n",
    "                  k.labels_.tolist(), k_ari]\n",
    "            \n",
    "            # Standard one layer AE1 \n",
    "            net = Autoencoder(in_feature=X.shape[1],embed_l=[1]).to(device)\n",
    "            ae_l = train_AE(net,X.clone(),Y.clone(), lr=lr,epochs=epochs)\n",
    "            X_embed = net.get_embedding(X).cpu().detach().numpy()\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=rep,n_init=rep).fit(X_embed)\n",
    "            AE_clust_assign = kmeans.labels_\n",
    "            ari = adjusted_rand_score(AE_clust_assign, Y.cpu())\n",
    "            ae1_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(ae1_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(ae_l[-1])\n",
    "            res.append(AE_clust_assign.tolist())\n",
    "            res.append(ari)\n",
    "            print('AE1 ', ari)\n",
    "\n",
    "            # Standard two layer AE2 \n",
    "            net = Autoencoder(in_feature=X.shape[1],embed_l=[2,1]).to(device)\n",
    "            ae_l = train_AE(net,X.clone(),Y.clone(), lr=lr,epochs=epochs)\n",
    "            X_embed = net.get_embedding(X).cpu().detach().numpy()\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=rep,n_init=rep).fit(X_embed)\n",
    "            AE_clust_assign = kmeans.labels_\n",
    "            ari = adjusted_rand_score(AE_clust_assign, Y.cpu())\n",
    "            ae2_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(ae2_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(ae_l[-1])\n",
    "            res.append(AE_clust_assign.tolist())\n",
    "            res.append(ari)\n",
    "            print('AE2 ', ari)\n",
    "\n",
    "            # Standard two layer AE3 \n",
    "            net = Autoencoder(in_feature=X.shape[1],embed_l=[2,1*n_clusters]).to(device)\n",
    "            ae_l = train_AE(net,X.clone(),Y.clone(), lr=lr,epochs=epochs)\n",
    "            X_embed = net.get_embedding(X).cpu().detach().numpy()\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=rep,n_init=rep).fit(X_embed)\n",
    "            AE_clust_assign = kmeans.labels_\n",
    "            ari = adjusted_rand_score(AE_clust_assign, Y.cpu())\n",
    "            ae3_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(ae3_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(ae_l[-1])\n",
    "            res.append(AE_clust_assign.tolist())\n",
    "            res.append(ari)\n",
    "            print('AE3 ', ari)\n",
    "\n",
    "            # Tensorized AE1 - one layer\n",
    "            net, tae_l, TAE_clust_assign, _, _, _ = train_TAE(X.clone(),Y.clone(), n_clusters=n_clusters, lr=lr, reg=0.1, embed_l=[1], shared=False, epochs=epochs)\n",
    "            TAE_clust_assign = torch.argmax(TAE_clust_assign,axis=0).cpu().detach().numpy()\n",
    "            ari = adjusted_rand_score(TAE_clust_assign, Y.cpu())\n",
    "            tae1_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(tae1_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(tae_l[-1])\n",
    "            res.append(TAE_clust_assign.tolist())\n",
    "            res.append(ari)\n",
    "            print('TAE1 ', ari)\n",
    "\n",
    "            # Tensorized AE2 - two layer\n",
    "            net, tae_l, TAE_clust_assign, _, _, _ = train_TAE(X.clone(),Y.clone(), n_clusters=n_clusters, lr=lr, reg=0.1, embed_l=[2,1], shared=False, epochs=epochs)\n",
    "            TAE_clust_assign = torch.argmax(TAE_clust_assign,axis=0).cpu().detach().numpy()\n",
    "            ari = adjusted_rand_score(TAE_clust_assign, Y.cpu())\n",
    "            tae2_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(tae2_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(tae_l[-1])\n",
    "            res.append(TAE_clust_assign.tolist())\n",
    "            res.append(ari)\n",
    "            print('TAE2 ', ari)\n",
    "\n",
    "            # Shared TAE\n",
    "            net, shared_l, SHARED_clust_assign, _, _, _ = train_TAE(X.clone(),Y.clone(), n_clusters=n_clusters, lr=lr, reg=0.1, epochs=epochs, embed_l=[2,1], shared=True)\n",
    "            PTAE_clust_assign = torch.argmax(SHARED_clust_assign,axis=0).cpu().detach().numpy()\n",
    "            ari = adjusted_rand_score(PTAE_clust_assign, Y.cpu())\n",
    "            ptae_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(ptae_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(shared_l[-1])\n",
    "            res.append(PTAE_clust_assign.tolist())\n",
    "            res.append(ari)\n",
    "            print('PTAE ', ari)\n",
    "\n",
    "            with open(csv_file, \"a\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcace8b",
   "metadata": {},
   "source": [
    "## Denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0423ef",
   "metadata": {},
   "source": [
    "### synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07686618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "shape of dataset:  torch.Size([150, 5])\n",
      "Number of parameters in AE:  10\n",
      "Number of parameters in AE:  24\n",
      "Number of parameters in AE:  32\n",
      "Number of parameters in TAE:  30\n",
      "epoch 99 loss 1.6224375376850366\n",
      "Number of parameters in TAE:  72\n",
      "epoch 99 loss 1.4371282520393531\n",
      "Number of parameters in TAE:  32\n",
      "epoch 99 loss 1.5511166723320882\n",
      "Number of parameters in AE:  10\n",
      "Number of parameters in AE:  24\n",
      "Number of parameters in AE:  32\n",
      "Number of parameters in TAE:  30\n",
      "epoch 99 loss 0.5540691858095428\n",
      "Number of parameters in TAE:  72\n",
      "epoch 99 loss 0.22249870121323814\n",
      "Number of parameters in TAE:  32\n",
      "epoch 99 loss 0.3822268884566923\n",
      "Number of parameters in AE:  10\n",
      "Number of parameters in AE:  24\n",
      "Number of parameters in AE:  32\n",
      "Number of parameters in TAE:  30\n",
      "epoch 99 loss 0.16377197625736395\n",
      "Number of parameters in TAE:  72\n",
      "epoch 99 loss 0.12183551267410318\n",
      "Number of parameters in TAE:  32\n",
      "epoch 99 loss 0.35368366612742347\n",
      "1\n",
      "shape of dataset:  torch.Size([150, 5])\n",
      "Number of parameters in AE:  10\n",
      "Number of parameters in AE:  24\n",
      "Number of parameters in AE:  32\n",
      "Number of parameters in TAE:  30\n",
      "epoch 99 loss 1.6817004988218347\n",
      "Number of parameters in TAE:  72\n",
      "epoch 99 loss 1.5157873591780662\n",
      "Number of parameters in TAE:  32\n",
      "epoch 99 loss 1.5575047435984015\n",
      "Number of parameters in AE:  10\n",
      "Number of parameters in AE:  24\n",
      "Number of parameters in AE:  32\n",
      "Number of parameters in TAE:  30\n",
      "epoch 99 loss 0.4911755985642473\n",
      "Number of parameters in TAE:  72\n",
      "epoch 99 loss 0.22920020494610072\n",
      "Number of parameters in TAE:  32\n",
      "epoch 99 loss 0.4089152252022177\n",
      "Number of parameters in AE:  10\n",
      "Number of parameters in AE:  24\n",
      "Number of parameters in AE:  32\n",
      "Number of parameters in TAE:  30\n",
      "epoch 99 loss 0.16336821259620288\n",
      "Number of parameters in TAE:  72\n",
      "epoch 99 loss 0.1209410954794536\n",
      "Number of parameters in TAE:  32\n",
      "epoch 99 loss 0.12392452599946409\n",
      "2\n",
      "shape of dataset:  torch.Size([150, 5])\n",
      "Number of parameters in AE:  10\n",
      "Number of parameters in AE:  24\n",
      "Number of parameters in AE:  32\n",
      "Number of parameters in TAE:  30\n",
      "epoch 99 loss 1.4777429615333677\n",
      "Number of parameters in TAE:  72\n",
      "epoch 99 loss 1.430763473138213\n",
      "Number of parameters in TAE:  32\n",
      "epoch 99 loss 1.5105469373861948\n",
      "Number of parameters in AE:  10\n",
      "Number of parameters in AE:  24\n",
      "Number of parameters in AE:  32\n",
      "Number of parameters in TAE:  30\n",
      "epoch 99 loss 0.4998333694071819\n",
      "Number of parameters in TAE:  72\n",
      "epoch 99 loss 0.21531221272113424\n",
      "Number of parameters in TAE:  32\n",
      "epoch 99 loss 0.5105007550958544\n",
      "Number of parameters in AE:  10\n",
      "Number of parameters in AE:  24\n",
      "Number of parameters in AE:  32\n",
      "Number of parameters in TAE:  30\n",
      "epoch 99 loss 0.16691160128451885\n",
      "Number of parameters in TAE:  72\n",
      "epoch 99 loss 0.11964431481280674\n",
      "Number of parameters in TAE:  32\n",
      "epoch 99 loss 0.1206987017331024\n",
      "3\n",
      "shape of dataset:  torch.Size([150, 5])\n",
      "Number of parameters in AE:  10\n",
      "Number of parameters in AE:  24\n",
      "Number of parameters in AE:  32\n",
      "Number of parameters in TAE:  30\n",
      "epoch 99 loss 1.3732750151927273\n",
      "Number of parameters in TAE:  72\n",
      "epoch 99 loss 1.62740687020123\n",
      "Number of parameters in TAE:  32\n",
      "epoch 99 loss 1.5825770481924215\n",
      "Number of parameters in AE:  10\n",
      "Number of parameters in AE:  24\n",
      "Number of parameters in AE:  32\n",
      "Number of parameters in TAE:  30\n",
      "epoch 99 loss 0.8658707565441728\n",
      "Number of parameters in TAE:  72\n",
      "epoch 99 loss 0.22046757698835184\n",
      "Number of parameters in TAE:  32\n",
      "epoch 99 loss 0.49786769221226373\n",
      "Number of parameters in AE:  10\n",
      "Number of parameters in AE:  24\n",
      "Number of parameters in AE:  32\n",
      "Number of parameters in TAE:  30\n",
      "epoch 99 loss 0.15975065793686857\n",
      "Number of parameters in TAE:  72\n",
      "epoch 99 loss 0.12133958574850112\n",
      "Number of parameters in TAE:  32\n",
      "epoch 99 loss 0.3181769797330101\n",
      "4\n",
      "shape of dataset:  torch.Size([150, 5])\n",
      "Number of parameters in AE:  10\n",
      "Number of parameters in AE:  24\n",
      "Number of parameters in AE:  32\n",
      "Number of parameters in TAE:  30\n",
      "epoch 99 loss 1.5637927624086538\n",
      "Number of parameters in TAE:  72\n",
      "epoch 99 loss 1.2322040813664594\n",
      "Number of parameters in TAE:  32\n",
      "epoch 99 loss 1.568791430356602\n",
      "Number of parameters in AE:  10\n",
      "Number of parameters in AE:  24\n",
      "Number of parameters in AE:  32\n",
      "Number of parameters in TAE:  30\n",
      "epoch 99 loss 0.5484292766451836\n",
      "Number of parameters in TAE:  72\n",
      "epoch 99 loss 0.21832236888042342\n",
      "Number of parameters in TAE:  32\n",
      "epoch 99 loss 0.30045336682349444\n",
      "Number of parameters in AE:  10\n",
      "Number of parameters in AE:  24\n",
      "Number of parameters in AE:  32\n",
      "Number of parameters in TAE:  30\n",
      "epoch 99 loss 0.15976586283339808\n",
      "Number of parameters in TAE:  72\n",
      "epoch 99 loss 0.1195565329818055\n",
      "Number of parameters in TAE:  32\n",
      "epoch 99 loss 0.3086350746142367\n",
      "0\n",
      "shape of dataset:  torch.Size([300, 6])\n",
      "Number of parameters in AE:  12\n",
      "Number of parameters in AE:  28\n",
      "Number of parameters in AE:  36\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.41591863410547375\n",
      "Number of parameters in TAE:  84\n",
      "epoch 99 loss 0.3918786643864587\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.40097481284523384\n",
      "Number of parameters in AE:  12\n",
      "Number of parameters in AE:  28\n",
      "Number of parameters in AE:  36\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.21616965271842978\n",
      "Number of parameters in TAE:  84\n",
      "epoch 99 loss 0.040464690909720956\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.15622878058658293\n",
      "Number of parameters in AE:  12\n",
      "Number of parameters in AE:  28\n",
      "Number of parameters in AE:  36\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.024695924761860322\n",
      "Number of parameters in TAE:  84\n",
      "epoch 99 loss 0.011470945179074382\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.106138536054641\n",
      "1\n",
      "shape of dataset:  torch.Size([300, 6])\n",
      "Number of parameters in AE:  12\n",
      "Number of parameters in AE:  28\n",
      "Number of parameters in AE:  36\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.3783324301475659\n",
      "Number of parameters in TAE:  84\n",
      "epoch 99 loss 0.3813379652099684\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.3721762862467828\n",
      "Number of parameters in AE:  12\n",
      "Number of parameters in AE:  28\n",
      "Number of parameters in AE:  36\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.20774279707344248\n",
      "Number of parameters in TAE:  84\n",
      "epoch 99 loss 0.046090657480526716\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.14550842702194738\n",
      "Number of parameters in AE:  12\n",
      "Number of parameters in AE:  28\n",
      "Number of parameters in AE:  36\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.027202077178129305\n",
      "Number of parameters in TAE:  84\n",
      "epoch 99 loss 0.011538184465607629\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.1244912711918975\n",
      "2\n",
      "shape of dataset:  torch.Size([300, 6])\n",
      "Number of parameters in AE:  12\n",
      "Number of parameters in AE:  28\n",
      "Number of parameters in AE:  36\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.5079095694748684\n",
      "Number of parameters in TAE:  84\n",
      "epoch 99 loss 0.3203140116343275\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.4270837313736168\n",
      "Number of parameters in AE:  12\n",
      "Number of parameters in AE:  28\n",
      "Number of parameters in AE:  36\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.11269768102094531\n",
      "Number of parameters in TAE:  84\n",
      "epoch 99 loss 0.050779995669145137\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.17802897664786238\n",
      "Number of parameters in AE:  12\n",
      "Number of parameters in AE:  28\n",
      "Number of parameters in AE:  36\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.02512233662690657\n",
      "Number of parameters in TAE:  84\n",
      "epoch 99 loss 0.011458586565374086\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.11258531482424587\n",
      "3\n",
      "shape of dataset:  torch.Size([300, 6])\n",
      "Number of parameters in AE:  12\n",
      "Number of parameters in AE:  28\n",
      "Number of parameters in AE:  36\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.4453638504150634\n",
      "Number of parameters in TAE:  84\n",
      "epoch 99 loss 0.38117089876749866\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.3942901679283629\n",
      "Number of parameters in AE:  12\n",
      "Number of parameters in AE:  28\n",
      "Number of parameters in AE:  36\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.22668679320253432\n",
      "Number of parameters in TAE:  84\n",
      "epoch 99 loss 0.04467405933498715\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.15396038678940385\n",
      "Number of parameters in AE:  12\n",
      "Number of parameters in AE:  28\n",
      "Number of parameters in AE:  36\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.028833102200490734\n",
      "Number of parameters in TAE:  84\n",
      "epoch 99 loss 0.011573180248378776\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.11343596331930408\n",
      "4\n",
      "shape of dataset:  torch.Size([300, 6])\n",
      "Number of parameters in AE:  12\n",
      "Number of parameters in AE:  28\n",
      "Number of parameters in AE:  36\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.4371079417131841\n",
      "Number of parameters in TAE:  84\n",
      "epoch 99 loss 0.3887421410724831\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.41435987086889026\n",
      "Number of parameters in AE:  12\n",
      "Number of parameters in AE:  28\n",
      "Number of parameters in AE:  36\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.24843249252997338\n",
      "Number of parameters in TAE:  84\n",
      "epoch 99 loss 0.04338673539226875\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.20811649974746008\n",
      "Number of parameters in AE:  12\n",
      "Number of parameters in AE:  28\n",
      "Number of parameters in AE:  36\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.026842339516151698\n",
      "Number of parameters in TAE:  84\n",
      "epoch 99 loss 0.11235123562704151\n",
      "Number of parameters in TAE:  36\n",
      "epoch 99 loss 0.11354116818634793\n"
     ]
    }
   ],
   "source": [
    "# kmeans_ari, ae_ari, tae_ari gives the ari of respective algorithms\n",
    "# hyperparameters: repetition, epochs, lr, reg,  \n",
    "repetition = 5\n",
    "results_dict = dict()\n",
    "\n",
    "csv_file = \"results/ae_denoising_synthetic.csv\"\n",
    "if not os.path.isfile(csv_file):\n",
    "    fields = [\"data\", \"rep\", \"X\", \"Y\", \"shape\", \"noise\",\n",
    "             \"ae1_num_params\", \"ae1_lr\", \"ae1_epochs\", \"ae1_final_train_loss\",\n",
    "             \"ae2_num_params\", \"ae2_lr\", \"ae2_epochs\", \"ae2_final_train_loss\", \n",
    "             \"ae3_num_params\", \"ae3_lr\", \"ae3_epochs\", \"ae3_final_train_loss\", \n",
    "             \"tae1_num_params\", \"tae1_lr\", \"tae1_epochs\", \"tae1_final_train_loss\",\n",
    "             \"tae2_num_params\", \"tae2_lr\", \"tae2_epochs\", \"tae2_final_train_loss\", \n",
    "             \"ptae_num_params\", \"ptae_lr\", \"ptae_epochs\", \"ptae_final_train_loss\", \n",
    "             ]\n",
    "    with open(csv_file, \"a\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(fields)\n",
    "\n",
    "lr_l = [0.001, 0.01, 0.1]\n",
    "epochs = 100\n",
    "noise = 0.1\n",
    "for (idx, dataset, data_generation) in [\n",
    "                            (0, 'parallel_line', parallel_line(noise=noise)), \n",
    "                            (1, 'orthogonal', orthogonal(noise=noise)), \n",
    "                            (2, 'triangle', triangle(noise=noise)), \n",
    "                            (3, 'lines_3D', lines_3D(noise=noise))\n",
    "                             ]:\n",
    "    for rep in range(repetition):\n",
    "        np.random.seed(rep)\n",
    "        print(rep)\n",
    "        X,Y,X_noise,n_clusters = data_generation\n",
    "        print(\"shape of dataset: \", X.shape)\n",
    "        x_idx = torch.tensor(np.arange(X.shape[0]))\n",
    "        shuffle_idx = torch.randperm(x_idx.shape[0])\n",
    "        x_idx = x_idx[shuffle_idx]\n",
    "        X_noise = X_noise[x_idx].float().to(device)\n",
    "        X = X[x_idx].float().to(device)\n",
    "        Y = Y[x_idx].float().to(device)\n",
    "         \n",
    "        for lr in lr_l:\n",
    "            res = [dataset, rep, X.cpu().detach().numpy().tolist(), Y.cpu().detach().numpy().tolist(), list(X.shape), noise,\n",
    "                  ]\n",
    "            \n",
    "            # Standard one layer AE1 \n",
    "            net = Autoencoder(in_feature=X.shape[1],embed_l=[1]).to(device)\n",
    "            ae_l = train_AE(net,X_noise.clone(),Y.clone(), lr=lr,epochs=epochs, X_out=X.clone())\n",
    "            ae1_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(ae1_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(ae_l[-1])\n",
    "\n",
    "            # Standard two layer AE2 \n",
    "            net = Autoencoder(in_feature=X.shape[1],embed_l=[2,1]).to(device)\n",
    "            ae_l = train_AE(net,X_noise.clone(),Y.clone(), lr=lr,epochs=epochs, X_out=X.clone())\n",
    "            ae2_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(ae2_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(ae_l[-1])\n",
    "\n",
    "            # Standard two layer AE3 \n",
    "            net = Autoencoder(in_feature=X.shape[1],embed_l=[2,1*n_clusters]).to(device)\n",
    "            ae_l = train_AE(net,X_noise.clone(),Y.clone(), lr=lr,epochs=epochs,X_out=X.clone())\n",
    "            ae3_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(ae3_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(ae_l[-1])\n",
    "\n",
    "            # Tensorized AE1 - one layer\n",
    "            net, tae_l, TAE_clust_assign, _, _, _ = train_TAE(X_noise.clone(),Y.clone(), n_clusters=n_clusters, lr=lr, reg=0.1, embed_l=[1], shared=False, epochs=epochs, X_out=X.clone())\n",
    "            tae1_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(tae1_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(tae_l[-1])\n",
    "\n",
    "            # Tensorized AE2 - two layer\n",
    "            net, tae_l, TAE_clust_assign, _, _, _ = train_TAE(X_noise.clone(),Y.clone(), n_clusters=n_clusters, lr=lr, reg=0.1, embed_l=[2,1], shared=False, epochs=epochs, X_out=X.clone())\n",
    "            tae2_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(tae2_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(tae_l[-1])\n",
    "\n",
    "            # Shared TAE\n",
    "            net, shared_l, SHARED_clust_assign, _, _, _ = train_TAE(X_noise.clone(),Y.clone(), n_clusters=n_clusters, lr=lr, reg=0.1, epochs=epochs, embed_l=[2,1], shared=True, X_out=X.clone())\n",
    "            ptae_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(ptae_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(shared_l[-1])\n",
    "\n",
    "            with open(csv_file, \"a\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ceb7b9",
   "metadata": {},
   "source": [
    "### real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46fed583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/lxc0D/apdl004/cluster_specific_representation/cluster_ae/src/data.py:102: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  peng = peng.replace({'species': peng_dict})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of dataset:  torch.Size([334, 4])\n",
      "4\n",
      "Number of parameters in AE:  8\n",
      "Number of parameters in AE:  20\n",
      "Number of parameters in AE:  28\n",
      "Number of parameters in TAE:  24\n",
      "epoch 199 loss 0.0027628246855397128\n",
      "Number of parameters in TAE:  60\n",
      "epoch 199 loss 0.002336736495613921\n",
      "Number of parameters in TAE:  28\n",
      "epoch 199 loss 0.002442312783038475\n",
      "Number of parameters in AE:  8\n",
      "Number of parameters in AE:  20\n",
      "Number of parameters in AE:  28\n",
      "Number of parameters in TAE:  24\n",
      "epoch 199 loss 0.0019748119421182674\n",
      "Number of parameters in TAE:  60\n",
      "epoch 199 loss 0.001893412104921421\n",
      "Number of parameters in TAE:  28\n",
      "epoch 199 loss 0.0018994574654466769\n",
      "Number of parameters in AE:  8\n",
      "Number of parameters in AE:  20\n",
      "Number of parameters in AE:  28\n",
      "Number of parameters in TAE:  24\n",
      "epoch 199 loss 0.0018962889833804178\n",
      "Number of parameters in TAE:  60\n",
      "epoch 199 loss 0.001892274151119462\n",
      "Number of parameters in TAE:  28\n",
      "epoch 199 loss 0.001885063430144257\n"
     ]
    }
   ],
   "source": [
    "# kmeans_ari, ae_ari, tae_ari gives the ari of respective algorithms\n",
    "# hyperparameters: repetition, epochs, lr, reg,  \n",
    "repetition = 5\n",
    "results_dict = dict()\n",
    "\n",
    "csv_file = \"results/ae_denoising_real.csv\"\n",
    "if not os.path.isfile(csv_file):\n",
    "    fields = [\"data\", \"rep\", \"X\", \"Y\", \"shape\", \"noise\",\n",
    "             \"ae1_num_params\", \"ae1_lr\", \"ae1_epochs\", \"ae1_final_train_loss\", \n",
    "             \"ae2_num_params\", \"ae2_lr\", \"ae2_epochs\", \"ae2_final_train_loss\",\n",
    "             \"ae3_num_params\", \"ae3_lr\", \"ae3_epochs\", \"ae3_final_train_loss\", \n",
    "             \"tae1_num_params\", \"tae1_lr\", \"tae1_epochs\", \"tae1_final_train_loss\", \n",
    "             \"tae2_num_params\", \"tae2_lr\", \"tae2_epochs\", \"tae2_final_train_loss\",\n",
    "             \"ptae_num_params\", \"ptae_lr\", \"ptae_epochs\", \"ptae_final_train_loss\", \n",
    "             ]\n",
    "    with open(csv_file, \"a\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(fields)\n",
    "\n",
    "lr_l = [0.001, 0.01, 0.1]\n",
    "epochs = 200\n",
    "noise = 0.1\n",
    "for dataset in ['mnist', 'penguin4', 'iris']:\n",
    "    if dataset == 'penguin4':\n",
    "        X,Y,X_noise,n_clusters = real_data_loader(dataset='penguin4', class_labels_list=[0,1,2], normalise_data=True, subsample=False, noise=noise)\n",
    "    elif dataset == 'iris':\n",
    "        X,Y,X_noise,n_clusters = real_data_loader(dataset='iris', class_labels_list=[0,1,2], normalise_data=False, subsample=False, noise=noise)\n",
    "    elif dataset == 'mnist':\n",
    "        X,Y,X_noise,n_clusters = real_data_loader(dataset='mnist', class_labels_list=[0,1,2,3,4], num_samples=200, normalise_data=False, subsample=True, noise=noise)\n",
    "    print(\"shape of dataset: \", X.shape)\n",
    "    for rep in range(repetition):\n",
    "        np.random.seed(rep)\n",
    "        print(rep)\n",
    "        x_idx = torch.tensor(np.arange(X.shape[0]))\n",
    "        shuffle_idx = torch.randperm(x_idx.shape[0])\n",
    "        x_idx = x_idx[shuffle_idx]\n",
    "        X_noise = X_noise[x_idx].float().to(device)\n",
    "        X = X[x_idx].float().to(device)\n",
    "        Y = Y[x_idx].float().to(device)\n",
    "         \n",
    "        for lr in lr_l:\n",
    "            res = [dataset, rep, X.cpu().detach().numpy().tolist(), Y.cpu().detach().numpy().tolist(), list(X.shape), noise,\n",
    "                  ]\n",
    "            \n",
    "            # Standard one layer AE1 \n",
    "            net = Autoencoder(in_feature=X.shape[1],embed_l=[1]).to(device)\n",
    "            ae_l = train_AE(net,X_noise.clone(),Y.clone(), lr=lr,epochs=epochs, X_out=X.clone())\n",
    "            ae1_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(ae1_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(ae_l[-1])\n",
    "\n",
    "            # Standard two layer AE2 \n",
    "            net = Autoencoder(in_feature=X.shape[1],embed_l=[2,1]).to(device)\n",
    "            ae_l = train_AE(net,X_noise.clone(),Y.clone(), lr=lr,epochs=epochs, X_out=X.clone())\n",
    "            ae2_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(ae2_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(ae_l[-1])\n",
    "\n",
    "            # Standard two layer AE3 \n",
    "            net = Autoencoder(in_feature=X.shape[1],embed_l=[2,1*n_clusters]).to(device)\n",
    "            ae_l = train_AE(net,X_noise.clone(),Y.clone(), lr=lr,epochs=epochs,X_out=X.clone())\n",
    "            ae3_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(ae3_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(ae_l[-1])\n",
    "\n",
    "            # Tensorized AE1 - one layer\n",
    "            net, tae_l, TAE_clust_assign, _, _, _ = train_TAE(X_noise.clone(),Y.clone(), n_clusters=n_clusters, lr=lr, reg=0.1, embed_l=[1], shared=False, epochs=epochs, X_out=X.clone())\n",
    "            tae1_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(tae1_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(tae_l[-1])\n",
    "\n",
    "            # Tensorized AE2 - two layer\n",
    "            net, tae_l, TAE_clust_assign, _, _, _ = train_TAE(X_noise.clone(),Y.clone(), n_clusters=n_clusters, lr=lr, reg=0.1, embed_l=[2,1], shared=False, epochs=epochs, X_out=X.clone())\n",
    "            tae2_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(tae2_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(tae_l[-1])\n",
    "\n",
    "            # Shared TAE\n",
    "            net, shared_l, SHARED_clust_assign, _, _, _ = train_TAE(X_noise.clone(),Y.clone(), n_clusters=n_clusters, lr=lr, reg=0.1, epochs=epochs, embed_l=[2,1], shared=True, X_out=X.clone())\n",
    "            ptae_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "            res.append(ptae_params)\n",
    "            res.append(lr)\n",
    "            res.append(epochs)\n",
    "            res.append(shared_l[-1])\n",
    "\n",
    "            with open(csv_file, \"a\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
